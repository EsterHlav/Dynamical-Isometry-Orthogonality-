{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Isometry As a Consequence of Weight Orthogonality for Faster and Better Convergence\n",
    "\n",
    "**Experimental part**\n",
    "\n",
    "Ester Hlav\n",
    "\n",
    "Columbia University\n",
    "\n",
    "\n",
    "Department of Mathematics\n",
    "\n",
    "May 2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, SimpleRNN, Dense, CuDNNLSTM, CuDNNGRU\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.initializers import Orthogonal\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "from numpy.linalg import svd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as transforms\n",
    "import pickle\n",
    "import flair\n",
    "import torch\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings\n",
    "import tqdm\n",
    "\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and process it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. For Sentiment Analysis task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file_embed(file, SEQ_LENGTH, embeddings='glove'):\n",
    "    \n",
    "    # define embedding\n",
    "    if embeddings == 'word2vec':\n",
    "        emb = WordEmbeddings('en')\n",
    "    elif embeddings == 'glove':\n",
    "        emb = WordEmbeddings('glove')\n",
    "    \n",
    "    padded = []\n",
    "    with open(file, 'r') as f:\n",
    "        for cnt, line in tqdm.tqdm_notebook(enumerate(f)):\n",
    "            # read line and embed\n",
    "            line = f.readline()\n",
    "            if line not in ['', ' ', '\\n']:\n",
    "                sent = Sentence(line)\n",
    "                emb.embed(sent)\n",
    "                sentvec = np.array([tok.embedding.numpy() for tok in sent])\n",
    "\n",
    "                # pad or cut to requested SEQ_LENGTH\n",
    "                padded.append(SEQ_LENGTH-sentvec.shape[0])\n",
    "                if sentvec.shape[0]>SEQ_LENGTH:\n",
    "                    sentvec = sentvec[:SEQ_LENGTH]\n",
    "                elif sentvec.shape[0]<SEQ_LENGTH:\n",
    "                    npad = ((0, SEQ_LENGTH-sentvec.shape[0]), (0, 0))\n",
    "                    sentvec = np.pad(sentvec, pad_width=npad, mode='constant', constant_values=0)\n",
    "                assert sentvec.shape[0] == SEQ_LENGTH\n",
    "\n",
    "                # reshape for staking\n",
    "                sentvec = sentvec.reshape((1, SEQ_LENGTH, -1))\n",
    "\n",
    "                # stack\n",
    "                if cnt == 0:\n",
    "                    # if first element then init\n",
    "                    sentences = sentvec\n",
    "                else:\n",
    "                    sentences = np.vstack((sentences, sentvec))\n",
    "    print ('Mean/std of padding: {}/{}'.format(np.mean(padded), np.std(padded)))\n",
    "    return (sentences)\n",
    "\n",
    "def get_sent_analysis(SEQ_LENGTH, embeddings='glove', percentValTest=[0.1, 0.1]):\n",
    "    \n",
    "    # read positive and negative sentences\n",
    "    pos_data = read_file_embed('rt-polarity.pos', SEQ_LENGTH, embeddings)\n",
    "    neg_data = read_file_embed('rt-polarity.neg', SEQ_LENGTH, embeddings)\n",
    "    X = np.vstack((pos_data, neg_data))\n",
    "    N = X.shape[0]\n",
    "    Y = np.zeros(N)\n",
    "    Y[:pos_data.shape[0]] = 1\n",
    "    \n",
    "    # shuffle dataset\n",
    "    indexes = np.random.choice(N, size=N, replace=False)\n",
    "    X = X[indexes]\n",
    "    Y = Y[indexes]\n",
    "    \n",
    "    # split train, val, test\n",
    "    ntrain = int(N*(1-sum(percentValTest)))\n",
    "    nval = int(N*(1-percentValTest[-1]))\n",
    "    X_train, X_val, X_test = X[:ntrain], X[ntrain:nval], X[nval:]\n",
    "    Y_train, Y_val, Y_test = Y[:ntrain], Y[ntrain:nval], Y[nval:]\n",
    "    return (X_train, Y_train, X_val, Y_val, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = get_sent_analysis(50, embeddings='word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in [X_train, Y_train, X_val, Y_val, X_test, Y_test]:\n",
    "    print (i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. For Sequential MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "def get_data_seqMNIST(VAL_PERCENT, normalization_input=True, bypixel=False):\n",
    "    # Load pre-shuffled MNIST data into train and test sets\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    if bypixel:\n",
    "        X_train = X_train.reshape(X_train.shape[0], 28*28, 1)\n",
    "        X_test = X_test.reshape(X_test.shape[0], 28*28, 1)\n",
    "\n",
    "    ntrain = int(X_train.shape[0]*(1-VAL_PERCENT))\n",
    "    X_train, X_val = X_train[:ntrain], X_train[ntrain:]\n",
    "    y_train, y_val = y_train[:ntrain], y_train[ntrain:]\n",
    "    \n",
    "    if normalization_input:\n",
    "        X_train = X_train.astype(\"float32\") / 255 #- 0.5\n",
    "        X_val = X_val.astype(\"float32\") / 255 #- 0.5\n",
    "        X_test = X_test.astype(\"float32\") / 255 #- 0.5\n",
    "        \n",
    "    Y_train = np_utils.to_categorical(y_train, 10)\n",
    "    Y_val = np_utils.to_categorical(y_val, 10)\n",
    "    Y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "    print (\"Shapes of X_train, X_val and X_test:\")\n",
    "    print (X_train.shape, X_val.shape, X_test.shape)\n",
    "    print (\"Shapes of Y_train, Y_val and Y_test:\")\n",
    "    print (Y_train.shape, Y_val.shape, Y_test.shape)\n",
    "    \n",
    "    return (X_train, Y_train, X_val, Y_val, X_test, Y_test)\n",
    "\n",
    "VAL_PERCENT = 0.2\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = get_data_seqMNIST(VAL_PERCENT, bypixel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define plotting function to visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_metric(result, names, config):\n",
    "    values = {}\n",
    "    for name in names:\n",
    "        values[name] = result[name]\n",
    "        \n",
    "    epochs = np.arange(len(values[names[0]]))\n",
    "    \n",
    "    plt.figure(figsize=(12,10))\n",
    "    for name in names:\n",
    "        plt.plot(epochs, values[name], label=name)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel(name)\n",
    "    plt.title(\"Evolution of {} during training of config {}\".format(\" and \".join(names), config))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_metrics(results, names):\n",
    "    values = {}\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'yellow', 'pink', 'purple', 'black', 'cyan']\n",
    "    configs = list(results.keys())\n",
    "    for config in configs:\n",
    "        values[config] = {}\n",
    "        for name in names:\n",
    "            values[config][name] = results[config][name]\n",
    "        \n",
    "    epochs = np.arange(len(values[configs[0]][names[0]]))\n",
    "    \n",
    "    fig, ax = plt.subplots(len(names), figsize=(12,10*len(names)))\n",
    "    for i, name in enumerate(names):\n",
    "        for j, config in enumerate(configs):\n",
    "            c = colors[j]\n",
    "            ax[i].plot(epochs, values[config][name], label=name+' '+config, color=c)\n",
    "        ax[i].set_xlabel('epochs')\n",
    "        ax[i].set_ylabel(name)\n",
    "        ax[i].set_title(\"Evolution of {} during training\".format(name))\n",
    "        ax[i].legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_sv(result, config):\n",
    "    svs = result['sv']\n",
    "    means = np.array([np.mean(sv) for sv in svs])\n",
    "    stds = np.array([np.std(sv) for sv in svs])\n",
    "    maxs = np.array([np.max(sv) for sv in svs])\n",
    "    mins = np.array([np.min(sv) for sv in svs])\n",
    "    last_sv = means[-1]\n",
    "    first_sv = means[0]\n",
    "    epochs = np.arange(means.shape[0])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,10))\n",
    "    # last sv\n",
    "    ax.axhline(y=last_sv, linestyle='--', color=\"black\", alpha=0.3)\n",
    "    trans = transforms.blended_transform_factory(\n",
    "    ax.get_yticklabels()[0].get_transform(), ax.transData)\n",
    "    ax.text(0, last_sv, \"{:.3f}\".format(last_sv), color=\"black\", transform=trans, \n",
    "            ha=\"right\")\n",
    "    # first sv\n",
    "    ax.axhline(y=first_sv, linestyle='--', color=\"black\", alpha=0.3)\n",
    "    trans = transforms.blended_transform_factory(\n",
    "    ax.get_yticklabels()[0].get_transform(), ax.transData)\n",
    "    ax.text(0, first_sv, \"{:.3f}\".format(first_sv), color=\"black\", transform=trans, \n",
    "            ha=\"right\")\n",
    "    plt.plot(epochs, means, label='mean', color='blue')\n",
    "    plt.fill_between(epochs, means-stds, means+stds, color='orange')\n",
    "    plt.plot(epochs, maxs, label='max', color='red')\n",
    "    plt.plot(epochs, mins, label='min', color='green')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('singular values')\n",
    "    plt.title('Evolution of statistics of singular values during training of config {}'.format(config))\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_svs(results):\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'yellow', 'pink', 'purple', 'black', 'cyan']\n",
    "    svss = {} \n",
    "    configs = list(results.keys())\n",
    "    for config in configs:\n",
    "        svss[config] = {}\n",
    "        svs = results[config]['sv']\n",
    "        svss[config]['means'] = np.array([np.mean(sv) for sv in svs])\n",
    "        svss[config]['stds'] = np.array([np.std(sv) for sv in svs])\n",
    "    epochs = np.arange(svss[list(results.keys())[0]]['means'].shape[0])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,10))\n",
    "    for i, config in enumerate(configs):\n",
    "        c = colors[i]\n",
    "        plt.plot(epochs, svss[config]['means'], label='mean {}'.format(config), color=c)\n",
    "        plt.fill_between(epochs, svss[config]['means']- svss[config]['stds'],  \n",
    "                         svss[config]['means']+ svss[config]['stds'], alpha=0.5, color=c)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('singular values')\n",
    "    plt.title('Evolution of statistics of singular values during training for different configs')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define gain-adjusted soft orthogonal regularizer, callback to track eigen values and the model building/training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NDIM = 128\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "TASK = 'sentiment' #'seqMNIST'#\n",
    "\n",
    "def orth_reg_gain(W, gain, reg_orth):\n",
    "    return reg_orth * K.mean(K.square(1/(gain**2) * K.dot(K.transpose(W), W)-1))\n",
    "\n",
    "class SingularValuesCallback(Callback):\n",
    "    def __init__(self, layer_name):\n",
    "        self.layer_name = layer_name\n",
    "        self.sv = []\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        layer_dict = dict([(layer.name, layer) for layer in self.model.layers])\n",
    "        self.layer = layer_dict[self.layer_name]\n",
    "        self.sv.append(svd(self.layer.get_weights()[1], compute_uv=False))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # compute singular values and save\n",
    "        svs = svd(self.layer.get_weights()[1], compute_uv=False)\n",
    "        self.sv.append(svs)\n",
    "\n",
    "class History(Callback):\n",
    "    \"\"\"Callback that records events into a `History` object.\n",
    "    This callback is automatically applied to\n",
    "    every Keras model. The `History` object\n",
    "    gets returned by the `fit` method of models.\n",
    "    Reinplemented to be able to compute loss at epoch 0, like for the sv\n",
    "    \"\"\"\n",
    "    def __init__(self, train_data, val_data):\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "def train(config, data='seqMNIST'): \n",
    "    model = Sequential()\n",
    "    if data == 'seqMNIST':\n",
    "        model.add(SimpleRNN(NDIM, return_sequences=False, \n",
    "              input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "              #activation = 'sigmoid',\n",
    "              recurrent_initializer=config['recurrent_initializer'],\n",
    "              recurrent_regularizer=config['recurrent_regularizer'], name='rnn_layer'))\n",
    "    elif data == 'sentiment':\n",
    "        model.add(CuDNNLSTM(NDIM, return_sequences=False, \n",
    "              input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "              recurrent_initializer=config['recurrent_initializer'],\n",
    "              recurrent_regularizer=config['recurrent_regularizer'], name='rnn_layer'))\n",
    "    #model.add(LSTM(64, return_sequences=False))\n",
    "\n",
    "    if data == 'seqMNIST':\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    elif data == 'sentiment':\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    svc = SingularValuesCallback('rnn_layer')\n",
    "    history = History([X_train, Y_train], [X_val, Y_val])\n",
    "    model.fit(X_train, Y_train, batch_size=BATCH_SIZE, \n",
    "                        epochs=EPOCHS, validation_data=(X_val, Y_val), callbacks=[history, svc])\n",
    "    \n",
    "    results = history.history\n",
    "    results['sv'] = svc.sv\n",
    "    \n",
    "    return (model, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define different configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'Orthogonal_Reg_0.01': {\n",
    "        'GAIN_ORTH': 1,\n",
    "        'recurrent_initializer': Orthogonal(gain=1.0),\n",
    "        'recurrent_regularizer': lambda x: orth_reg_gain(x, 1.0, 0.01)\n",
    "    }, \n",
    "    'Orthogonal_Reg_0.1': {\n",
    "        'GAIN_ORTH': 1,\n",
    "        'recurrent_initializer': Orthogonal(gain=1.0),\n",
    "        'recurrent_regularizer': lambda x: orth_reg_gain(x, 1.0, 0.1)\n",
    "    }, \n",
    "    'Orthogonal_Reg_0.5': {\n",
    "        'GAIN_ORTH': 1,\n",
    "        'recurrent_initializer': Orthogonal(gain=1.0),\n",
    "        'recurrent_regularizer': lambda x: orth_reg_gain(x, 1.0, 0.5)\n",
    "    }, \n",
    "    'Orthogonal_NoReg': {\n",
    "        'GAIN_ORTH': 1,\n",
    "        'recurrent_initializer': Orthogonal(gain=1.0),\n",
    "        'recurrent_regularizer': None\n",
    "    },\n",
    "    'NoOrthogonal': {\n",
    "        'GAIN_ORTH': 1,\n",
    "        'recurrent_initializer': 'glorot_uniform',\n",
    "        'recurrent_regularizer': None\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GAIN = 1.025\n",
    "\n",
    "configs2 = {\n",
    "    'Orthogonal_Gain{}_adj_Reg1'.format(GAIN): {\n",
    "        'GAIN_ORTH': GAIN,\n",
    "        'recurrent_initializer': Orthogonal(gain=GAIN),\n",
    "        'recurrent_regularizer': lambda x: orth_reg_gain(x, GAIN, 1)\n",
    "    },\n",
    "    'Orthogonal_Gain{}_adj_Reg0.5'.format(GAIN): {\n",
    "        'GAIN_ORTH': GAIN,\n",
    "        'recurrent_initializer': Orthogonal(gain=GAIN),\n",
    "        'recurrent_regularizer': lambda x: orth_reg_gain(x, GAIN, 0.5)\n",
    "    },\n",
    "    'Orthogonal_Gain{}_Reg1'.format(GAIN): {\n",
    "        'GAIN_ORTH': GAIN,\n",
    "        'recurrent_initializer': Orthogonal(gain=GAIN),\n",
    "        'recurrent_regularizer': lambda x: orth_reg_gain(x, 1.00, 1)\n",
    "    },\n",
    "    'Orthogonal_Gain{}_Reg0.5'.format(GAIN): {\n",
    "        'GAIN_ORTH': GAIN,\n",
    "        'recurrent_initializer': Orthogonal(gain=GAIN),\n",
    "        'recurrent_regularizer': lambda x: orth_reg_gain(x, 1.00, 0.5)\n",
    "    },\n",
    "    'Orthogonal_NoReg': {\n",
    "        'GAIN_ORTH': 1,\n",
    "        'recurrent_initializer': Orthogonal(gain=1.0),\n",
    "        'recurrent_regularizer': None\n",
    "    },\n",
    "    'NoOrthogonal': {\n",
    "        'GAIN_ORTH': 1,\n",
    "        'recurrent_initializer': 'glorot_normal',\n",
    "        'recurrent_regularizer': None\n",
    "    }  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GAIN = 1.25\n",
    "\n",
    "configs3 = {\n",
    "    'Orthogonal_Gain{}_adj_Reg1'.format(GAIN): {\n",
    "        'GAIN_ORTH': GAIN,\n",
    "        'recurrent_initializer': Orthogonal(gain=GAIN),\n",
    "        'recurrent_regularizer': lambda x: orth_reg_gain(x, GAIN, 1)\n",
    "    },\n",
    "    'Orthogonal_Gain{}_adj_Reg0.5'.format(GAIN): {\n",
    "        'GAIN_ORTH': GAIN,\n",
    "        'recurrent_initializer': Orthogonal(gain=GAIN),\n",
    "        'recurrent_regularizer': lambda x: orth_reg_gain(x, GAIN, 0.5)\n",
    "    },\n",
    "    'Orthogonal_Gain{}_Reg1'.format(GAIN): {\n",
    "        'GAIN_ORTH': GAIN,\n",
    "        'recurrent_initializer': Orthogonal(gain=GAIN),\n",
    "        'recurrent_regularizer': lambda x: orth_reg_gain(x, 1.00, 1)\n",
    "    },\n",
    "    'Orthogonal_Gain{}_Reg0.5'.format(GAIN): {\n",
    "        'GAIN_ORTH': GAIN,\n",
    "        'recurrent_initializer': Orthogonal(gain=GAIN),\n",
    "        'recurrent_regularizer': lambda x: orth_reg_gain(x, 1.00, 0.5)\n",
    "    },\n",
    "    'Orthogonal_Gain{}_NoReg'.format(GAIN): {\n",
    "        'GAIN_ORTH': GAIN,\n",
    "        'recurrent_initializer': Orthogonal(gain=GAIN),\n",
    "        'recurrent_regularizer': None\n",
    "    },\n",
    "    'Orthogonal_NoReg': {\n",
    "        'GAIN_ORTH': 1,\n",
    "        'recurrent_initializer': Orthogonal(gain=1.0),\n",
    "        'recurrent_regularizer': None\n",
    "    },\n",
    "    'NoOrthogonal': {\n",
    "        'GAIN_ORTH': 1,\n",
    "        'recurrent_initializer': 'glorot_uniform',\n",
    "        'recurrent_regularizer': None\n",
    "    }  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training for the configurations picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfgs = configs3\n",
    "results = {}\n",
    "for config in cfgs.keys():\n",
    "    print (\"###\"*20)\n",
    "    print (\"Starting training of config {}\".format(config))\n",
    "    print (\"###\"*20)\n",
    "    model, result = train(cfgs[config], TASK)\n",
    "    results[config] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_svs(results)\n",
    "plot_metrics(results, ['loss', 'val_loss'])\n",
    "plot_metrics(results, ['acc', 'val_acc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
